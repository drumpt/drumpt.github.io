---
title: "Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody Prompting"
authors:
  - Wooseok Han
  - Minki Kang
  - admin
  - Eunho Yang
author_notes:
  - "Equal contribution"
  - "Equal contribution"
date: "2024-10-01T00:00:00Z"
publishDate: "1998-03-20T00:00:00Z"
publication_types: ["3"]
publication: IEEE International Conference on Acoustics, Speech and Signal Processing (**ICASSP**), 2025
publication_short: ICASSP 2025

url_pdf:
url_preprint: 'https://arxiv.org/abs/2412.20155'
url_code:
image:
  caption: "Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)"
  focal_point: ""
  preview_only: false

note:
abstract: Speaker-adaptive Text-to-Speech (TTS) synthesis has attracted considerable attention due to its broad range of applications, such as personalized voice assistant services. While several approaches have been proposed, they often exhibit high sensitivity to either the quantity or the quality of target speech samples. To address these limitations, we introduce Stable-TTS, a novel speaker-adaptive TTS framework that leverages a small subset of a high-quality pre-training dataset, referred to as prior samples. Specifically, Stable-TTS achieves prosody consistency by leveraging the high-quality prosody of prior samples, while effectively capturing the timbre of the target speaker. Additionally, it employs a prior-preservation loss during fine-tuning to maintain the synthesis ability for prior samples to prevent overfitting on target samples. Extensive experiments demonstrate the effectiveness of Stable-TTS even under limited target speech samples.
summary:
tags: []
featured: true
---

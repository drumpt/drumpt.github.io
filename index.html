<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: February 1, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.018058c879a481473e1c076a8e11c2e5.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Changhun Kim"><meta name=description content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><link rel=alternate hreflang=en-us href=https://drumpt.github.io/><link rel=canonical href=https://drumpt.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu3c06ff49a96dc5c4160caa1f4cb0f97d_35271_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu3c06ff49a96dc5c4160caa1f4cb0f97d_35271_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#2c6941"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://drumpt.github.io/media/icon_hu3c06ff49a96dc5c4160caa1f4cb0f97d_35271_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Changhun Kim"><meta property="og:url" content="https://drumpt.github.io/"><meta property="og:title" content="Changhun Kim"><meta property="og:description" content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><meta property="og:image" content="https://drumpt.github.io/media/icon_hu3c06ff49a96dc5c4160caa1f4cb0f97d_35271_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-10-24T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://drumpt.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://drumpt.github.io/"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Changhun Kim"><title>Changhun Kim</title><script async defer src=https://buttons.github.io/buttons.js></script></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper data-wc-page-id=3976528693a0108357f4928017600865><script src=/js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Changhun Kim</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Changhun Kim</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#news data-target=#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#experience data-target=#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#patents data-target=#patents><span>Patents</span></a></li><li class=nav-item><a class=nav-link href=/#honors data-target=#honors><span>Honors</span></a></li><li class=nav-item><a class=nav-link href=/#teaching data-target=#teaching><span>Teaching</span></a></li><li class=nav-item><a class=nav-link href=/#service data-target=#service><span>Service</span></a></li><li class=nav-item><a class=nav-link href=/#projects data-target=#projects><span>Projects</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-github-drumpt-about"><div class=home-section-bg></div><div class=container><link rel=stylesheet type=text/css rel=noopener target=_blank href=github.drumpt.about.css><br><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/authors/admin/avatar_hu4ffaf8ee58ba3407b626001937d806ef_226814_270x270_fill_q75_lanczos_center.jpg alt="Changhun Kim"><div class=portrait-title><h2><div style=font-weight:400>Changhun Kim</div></h2><h3 style=font-weight:400>Machine Learning Researcher</h3><h3><a href="https://en.aitrics.com/?redirect=no" target=_blank rel=noopener><span style=font-weight:400>AITRICS</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=mailto:changhun.a.kim@gmail.com aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?user=mY7KEvAAAAAJ" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar big-icon"></i></a></li><li><a href=https://github.com/drumpt target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=https://www.linkedin.com/in/changhun-kim/ target=_blank rel=noopener aria-label=linkedin><i class="fab fa-linkedin big-icon"></i></a></li><li><a href=https://twitter.com/hooni_ne target=_blank rel=noopener aria-label=x><i class="fab fa-x big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><div class=article-style style=text-align:justify><p>I am a <strong>machine learning researcher</strong> at <a href="http://en.aitrics.com/?redirect=no" target=_blank rel=noopener>AITRICS</a>, a healthcare AI startup in Korea, where I am fulfilling my <strong>alternative military service</strong> under Prof. <a href=https://mli.kaist.ac.kr/people/ target=_blank rel=noopener>Eunho Yang</a>. Before joining AITRICS, I obtained my master’s degree in <strong>Artificial Intelligence</strong> from <a href=https://kaist.ac.kr/en/ target=_blank rel=noopener>KAIST</a>, also under the guidance of Prof. Eunho Yang. I completed my bachelor’s degree in <strong>Computer Science</strong> and <strong>Mathematical Sciences</strong> at KAIST as well. I am open to <strong>research collaborations globally, including remote opportunities</strong>. If you are interested in my work, please feel free to reach out!</p><p>My long-term research objective is to <strong>enhance the out-of-distribution generalization capability of machine learning models, thereby creating trustworthy AI systems that can be reliably deployed in new environments</strong>. This multifaceted goal involves ensuring robustness to distribution shifts (<strong>domain adaptation and generalization</strong>), handling unseen labels (<strong>zero-shot learning</strong>), and managing unseen tasks (<strong>cross-task generalization</strong>). During my master’s program, I focused on <strong>test-time adaptation to distribution shifts across various tasks</strong>, including 3D point cloud recognition, zero-shot transfer of vision-language models, automatic speech recognition, tabular learning, and time series classification. These experiences have also <strong>enabled myself to quickly adapt to new modalities and tasks</strong>.</p><p>At AITRICS, my research centers on <strong>improving the generalizability, robustness, and explainability of early prediction models for critical clinical outcomes</strong> such as cardiac arrests and in-hospital mortality. I am also interested in <strong>parameter- and data-efficient adaptation of deep generative models</strong>, such as <strong>diffusion models</strong> and <strong>large multimodal models</strong>, to downstream tasks. To achieve these goals, I work on <strong>developing practical algorithms that are empirically well-grounded or theoretically provable</strong>. Additionally, I am passionate about <strong>providing theoretical insights into machine learning models</strong> through the lens of probabilistic (<strong>Bayesian inference</strong>) and statistical (<strong>generalization bounds</strong>) frameworks.</p></div><div class=row><div class=col-md-auto style=width:50%><div class=section-subheading>Contact</div><ul class="ul-contact fa-ul mb-0" style=--fa-li-margin:1.5em><li><i class="fa-li fa-solid fa-location-pin"></i>
AP Tower 13F, 218, Teheran-ro, Gangnam-gu, Seoul 06221, Korea</li><li><i class="fa-li fa-solid fa-phone"></i>
(+82) 10-3264-6509</li><li><i class="fa-li fa-solid fa-envelope"></i>
changhun.a.kim@gmail.com</li></ul></div><div class=col-md-auto style=width:50%><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0" style=--fa-li-margin:1.5em><li><i class="fa-li fa-solid fa-graduation-cap"></i><div class=description><p class=course>M.S. in Artificial Intelligence, 2024</p><p class=institution>Korea Advanced Institute of Science and Technology (KAIST)</p></div></li><li><i class="fa-li fa-solid fa-graduation-cap"></i><div class=description><p class=course>B.S. in Computer Science and Mathematical Sciences, 2022</p><p class=institution>Korea Advanced Institute of Science and Technology (KAIST)</p></div></li></ul></div></div></div></div></div></div></section><section id=news class="home-section wg-github-drumpt-news"><div class=home-section-bg></div><div class=container><div class=section-subheading>News<hr></div><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Dec 2024]</span>
One paper is accepted to ICASSP 2025.</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Oct 2024]</span>
One paper is accepted to NeurIPSW-TRL 2024.</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Sep 2024]</span>
I am attending ECCV 2024 in person. See you in Milano!</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Jul 2024]</span>
One paper is accepted to ECCV 2024.</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Jun 2024]</span>
I am attending KOSMI 2024 in person. See you in Seoul!</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Apr 2024]</span>
I am attending ICASSP 2024 in person. See you in Seoul!</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Feb 2024]</span>
I have completed my master’s thesis defense and graduated from my master's program.</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Nov 2023]</span>
I am starting my next research journey as a machine learning researcher at AITRICS.</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[Aug 2023]</span>
I am attending INTERSPEECH 2023 in person. See you in Dublin!</li></ul><ul style=margin-bottom:0;padding-inline-start:15px><li style=list-style-position:outside><span style=font-weight:700>[May 2023]</span>
One paper is accepted to INTERSPEECH 2023 as an oral presentation.</li></ul></div></section><section id=experience class="home-section wg-github-drumpt-experience"><div class=home-section-bg></div><div class=container><div class=section-subheading>Professional Experience</div><hr><div class=pub-content-container><div class=pub-image-container><a href=https://www.aitrics.com/en/ target=_blank rel=noopener><img src=/media/icons/brands/aitrics.png width=56 height=56 alt=AITRICS loading=lazy></a></div><div class=pub-text-container><span style=font-weight:700><a href=https://www.aitrics.com/en/ target=_blank rel=noopener>AITRICS</a></span>,
Seoul, Korea<br><span style=font-weight:700>Machine Learning Researcher</span>,
Nov 2023–Present<br><br></div></div><div class=pub-content-container><div class=pub-image-container><a href=https://mli.kaist.ac.kr/ target=_blank rel=noopener><img src=/media/icons/brands/mlilab.png width=56 height=56 alt="KAIST Machine Learning and Intelligence Lab" loading=lazy></a></div><div class=pub-text-container><span style=font-weight:700><a href=https://mli.kaist.ac.kr/ target=_blank rel=noopener>KAIST Machine Learning and Intelligence Lab</a></span>,
Daejeon, Korea<br><span style=font-weight:700>Master&rsquo;s Student Researcher</span>,
Mar 2022–Feb 2024<br><span style=font-weight:700>Undergraduate Researcher</span>,
Jun 2021–Feb 2022<br><br></div></div><div class=pub-content-container><div class=pub-image-container><a href=https://deepnatural.ai/ target=_blank rel=noopener><img src=/media/icons/brands/deepnatural.png width=56 height=56 alt=DeepNatural loading=lazy></a></div><div class=pub-text-container><span style=font-weight:700><a href=https://deepnatural.ai/ target=_blank rel=noopener>DeepNatural</a></span>,
Seoul, Korea<br><span style=font-weight:700>Machine Learning Engineer</span>,
Sep 2020–Feb 2021<br><br></div></div><div class=pub-content-container><div class=pub-image-container><a href=https://vil.kaist.ac.kr/ target=_blank rel=noopener><img src=/media/icons/brands/vil.png width=56 height=56 alt="KAIST Vehicular Intelligence Lab" loading=lazy></a></div><div class=pub-text-container><span style=font-weight:700><a href=https://vil.kaist.ac.kr/ target=_blank rel=noopener>KAIST Vehicular Intelligence Lab</a></span>,
Daejeon, Korea<br><span style=font-weight:700>Undergraduate Researcher</span>,
Oct 2019–Aug 2020<br><br></div></div><div class=pub-content-container><div class=pub-image-container><a href=https://www.netmarble.net/ target=_blank rel=noopener><img src=/media/icons/brands/netmarble.png width=56 height=56 alt=Netmarble loading=lazy></a></div><div class=pub-text-container><span style=font-weight:700><a href=https://www.netmarble.net/ target=_blank rel=noopener>Netmarble</a></span>,
Seoul, Korea<br><span style=font-weight:700>Data Engineer</span>,
Jun 2019–Aug 2019</div></div><style>.pub-content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.pub-image-container{flex:0 0 50px;margin-right:1rem}.pub-text-container{flex:1}</style></div></section><section id=publications class="home-section wg-github-drumpt-news"><div class=home-section-bg></div><div class=container><div class=section-subheading>Publications<hr></div><div class=content-container><div class=image-container><img alt="Timing: Temporality-Aware Integrated Gradients for Time Series Explanation" class=transition-all data-nimg=fill decoding=async fetchpriority=high loading=lazy src=/publications/timing/featured.png style=max-width:100%;height:auto;object-fit:contain></div><div class=text-container><span style=font-weight:700>Timing: Temporality-Aware Integrated Gradients for Time Series Explanation</span><br><span class="article-metadata li-cite-author"><span>Hyeongwon Jang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Changhun Kim</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Eunho Yang</span></span><br>Manuscript, 2025<br></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.image-container{flex:0 0 250px;margin-right:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.image-container{flex:none;margin-right:0;margin-bottom:1rem;width:75%}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style><div class=content-container><div class=image-container><img alt="SPAM: Sampling Pattern Meta-Learning for Domain Generalization on Irregular Time Series" class=transition-all data-nimg=fill decoding=async fetchpriority=high loading=lazy src=/publications/spam/featured.png style=max-width:100%;height:auto;object-fit:contain></div><div class=text-container><span style=font-weight:700>SPAM: Sampling Pattern Meta-Learning for Domain Generalization on Irregular Time Series</span><br><span class="article-metadata li-cite-author"><span class=author-highlighted>Changhun Kim</span>, <span>Joohyung Lee</span>, <span>Kwanhyung Lee</span>, <span>Donghwee Yoon</span>, <span>Eunho Yang</span></span><br>Manuscript, 2025<br></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.image-container{flex:0 0 250px;margin-right:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.image-container{flex:none;margin-right:0;margin-bottom:1rem;width:75%}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style><div class=content-container><div class=image-container><img alt="Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody Prompting" class=transition-all data-nimg=fill decoding=async fetchpriority=high loading=lazy src=/publications/stable-tts/featured.png style=max-width:100%;height:auto;object-fit:contain></div><div class=text-container><span style=font-weight:700>Stable-TTS: Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody Prompting</span><br><span class="article-metadata li-cite-author"><span>Wooseok Han</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Minki Kang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Changhun Kim</span>, <span>Eunho Yang</span></span><br>IEEE International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP</strong>), 2025<br><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2412.20155 target=_blank rel=noopener>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://stable-tts.github.io target=_blank rel=noopener>Project</a></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.image-container{flex:0 0 250px;margin-right:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.image-container{flex:none;margin-right:0;margin-bottom:1rem;width:75%}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style><div class=content-container><div class=image-container><img alt="AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler" class=transition-all data-nimg=fill decoding=async fetchpriority=high loading=lazy src=/publications/adaptable/featured.png style=max-width:100%;height:auto;object-fit:contain></div><div class=text-container><span style=font-weight:700>AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler</span><br><span class="article-metadata li-cite-author"><span class=author-highlighted>Changhun Kim</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Taewon Kim</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Seungyeon Woo</span>, <span>June Yong Yang</span>, <span>Eunho Yang</span></span><br>NeurIPS Workshop on Table Representation Learning (<strong>NeurIPSW-TRL</strong>), 2024<br><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=yhD2icebZ0#discussion" target=_blank rel=noopener>Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2407.10784 target=_blank rel=noopener>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/drumpt/AdapTable target=_blank rel=noopener>Code</a></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.image-container{flex:0 0 250px;margin-right:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.image-container{flex:none;margin-right:0;margin-bottom:1rem;width:75%}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style><div class=content-container><div class=image-container><img alt="CloudFixer: Test-Time Adaptation for 3D Point Clouds via Diffusion-Guided Geometric Transformation" class=transition-all data-nimg=fill decoding=async fetchpriority=high loading=lazy src=/publications/cloudfixer/featured.png style=max-width:100%;height:auto;object-fit:contain></div><div class=text-container><span style=font-weight:700>CloudFixer: Test-Time Adaptation for 3D Point Clouds via Diffusion-Guided Geometric Transformation</span><br><span class="article-metadata li-cite-author"><span>Hajin Shim</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Changhun Kim</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Eunho Yang</span></span><br>European Conference on Computer Vision (<strong>ECCV</strong>), 2024<br><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4603_ECCV_2024_paper.php target=_blank rel=noopener>Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2407.16193 target=_blank rel=noopener>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/shimazing/CloudFixer target=_blank rel=noopener>Code</a></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.image-container{flex:0 0 250px;margin-right:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.image-container{flex:none;margin-right:0;margin-bottom:1rem;width:75%}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style><div class=content-container><div class=image-container><img alt="SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization" class=transition-all data-nimg=fill decoding=async fetchpriority=high loading=lazy src=/publications/sgem/featured.png style=max-width:100%;height:auto;object-fit:contain></div><div class=text-container><span style=font-weight:700>SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization</span><br><span class="article-metadata li-cite-author"><span class=author-highlighted>Changhun Kim</span>, <span>Joonhyung Park</span>, <span>Hajin Shim</span>, <span>Eunho Yang</span></span><br>Conference of the International Speech Communication Association (<strong>INTERSPEECH</strong>), 2023, <span style=font-weight:700>Oral Presentation</span><br><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.isca-archive.org/interspeech_2023/kim23f_interspeech.html target=_blank rel=noopener>Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2306.01981 target=_blank rel=noopener>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/drumpt/SGEM target=_blank rel=noopener>Code</a></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.image-container{flex:0 0 250px;margin-right:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.image-container{flex:none;margin-right:0;margin-bottom:1rem;width:75%}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style></div></section><section id=patents class="home-section wg-github-drumpt-news"><div class=home-section-bg></div><div class=container><div class=section-subheading>Patents<hr></div><div class=content-container><div class=text-container><span style=font-weight:700>Stable Speaker-Adaptive Text-to-Speech Synthesis via Prosody Prompting</span><br><span class="article-metadata li-cite-author"><span>Wooseok Han</span>, <span>Minki Kang</span>, <span class=author-highlighted>Changhun Kim</span>, <span>Eunho Yang</span>, <span>Kwang Joon Kim</span></span><br>KR Patent App. 10-2024-0006413, 2025<br></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style><div class=content-container><div class=text-container><span style=font-weight:700>Method for Providing Explanation for Patient State Prediction and Electronic Apparatus Therefor</span><br><span class="article-metadata li-cite-author"><span class=author-highlighted>Changhun Kim</span>, <span>Sangchul Hahn</span>, <span>Kwang Joon Kim</span></span><br>KR Patent App. 10-2025-0009664, 2025<br></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style><div class=content-container><div class=text-container><span style=font-weight:700>Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization</span><br><span class="article-metadata li-cite-author"><span>Eunho Yang</span>, <span class=author-highlighted>Changhun Kim</span>, <span>Joonhyung Park</span>, <span>Hajin Shim</span></span><br>US Patent App. 18/594,442, 2024<br>KR Patent App. 10-2024-0006413, 2024<br>KR Patent App. 10-2024-0023266, 2024<br></div></div><style>.content-container{display:flex;flex-wrap:wrap;margin-top:1rem}.text-container{flex:1}.placeholder{width:100%;height:0;padding-bottom:56.25%;background-color:transparent}@media(max-width:768px){.content-container{flex-direction:column;align-items:center}.text-container{width:100%;text-align:left}.placeholder{display:none}}</style></div></section><section id=honors class="home-section wg-github-drumpt-others"><div class=home-section-bg></div><div class=container><div class=section-subheading>Honors and Awards</div><hr><span style=font-weight:700>Dongwon Full Masters Scholarship</span><br>Dongwon Group, Spring 2022–Fall 2023<br><br><span style=font-weight:700>Magna Cum Laude</span><br>KAIST College of Engineering, Feb 2022<br><br><span style=font-weight:700>Silver Prize</span><br>Korean Undergraduate Mathematics Competition, Korean Mathematics Society, Jan 2022<br><br><span style=font-weight:700>Overseas Exchange Scholarship</span><br>Mirae Asset, Dec 2019<br><br><span style=font-weight:700>Representative of Student Exchange Ambassador</span><br>KAIST, Nov 2019<br><br><span style=font-weight:700>Honor Student</span><br>KAIST College of Engineering, Sep 2019<br><br><span style=font-weight:700>KAIST Convergence AMP Scholarship</span><br>KAIST School of Computing, Mar 2019<br><br><span style=font-weight:700>Winner</span><br>Science Quiz, KAIST-POSTECH Science War, Sep 2018<br><br><span style=font-weight:700>National Full Undergraduate Scholarship</span><br>Korea Student Aid Foundation, Spring 2017–Fall 2021<br></div></section><section id=teaching class="home-section wg-github-drumpt-others"><div class=home-section-bg></div><div class=container><div class=section-subheading>Teaching Experience</div><hr><span style=font-weight:700>Teaching Assistant</span><br>Tabular Learning, Hanwha Ocean Capstone Project, Spring 2023<br><br><span style=font-weight:700>Teaching Assistant</span><br>AI Soccer Challenge, Bokja Girls&rsquo; High School AI Education Program, Fall 2020<br></div></section><section id=service class="home-section wg-github-drumpt-others"><div class=home-section-bg></div><div class=container><div class=section-subheading>Academic Service</div><hr><span style=font-weight:700>Journal Reviewer</span><br><ul style=margin-bottom:0><li>Transactions on Machine Learning Research (<strong>TMLR</strong>), 2024</li><li>IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>), 2025</li></ul><br><span style=font-weight:700>Conference Reviewer</span><br><ul style=margin-bottom:0><li>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024</li><li>International Conference on Machine Learning (<strong>ICML</strong>), 2025</li><li>International Conference on Learning Representations (<strong>ICLR</strong>), 2025</li><li>International Joint Conferences on Artificial Intelligence (<strong>IJCAI</strong>), 2025</li><li>IEEE International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP</strong>), 2025</li><li>Learning on Graphs Conference (<strong>LoG</strong>), 2024</li></ul></div></section></div><div class=page-footer><div class=container><footer class=site-footer></footer></div></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.3322c0d94f0e691b0b24c63f4c41064b.js></script></body></html>